Lab Experiment 1: Data Visualization
Objective: To construct various types of plots and charts, such as histograms, bar charts, pie
charts, and scatter plots, by importing data from a CSV file. Further, to label different axes and
data within the plots for better readability and interpretation.
1. Introduction & Theory
Data visualization is a fundamental aspect of machine learning and data analysis. It allows us
to understand the underlying patterns, distributions, and relationships within a dataset.
Visualizing data helps in identifying outliers, understanding feature distributions, and
communicating insights effectively.
Matplotlib is a comprehensive library for creating static, animated, and interactive
visualizations in Python. It provides a wide variety of plots and charts. In this experiment, we
will use Matplotlib in conjunction with the Pandas library to perform basic data visualization
tasks on a sample dataset. We will explore the following key plot types:
â€¢ Histogram: To understand the distribution of a single numerical variable.
â€¢ Bar Chart: To compare categorical data.
â€¢ Pie Chart: To show the proportion of each category in a whole.
â€¢ Scatter Plot: To visualize the relationship between two numerical variables.
Conclusion
This experiment successfully demonstrated the use of the Matplotlib library for creating
fundamental data visualizations. We generated a sample dataset using Pandas and proceeded to
create, label, and customize histograms, bar charts, pie charts, and scatter plots. This process is
crucial for the initial exploratory data analysis (EDA) phase in any machine learning project,
as it provides immediate insights into the data's structure and characteristics.
import random
import pandas as pd
import matplotlib.pyplot as plt

data = {
    'age': [random.randint(20, 60) for _ in range(100)],
    'gender': [random.choice(['Male', 'Female']) for _ in range(100)],
    'income': [random.randint(20000, 100000) for _ in range(100)]
}
df = pd.DataFrame(data)
df.to_csv('data.csv', index=False)

# Histogram
plt.figure(figsize=(6, 4))
plt.hist(df['age'], bins=10, edgecolor='black')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Age Distribution')

# Green Histogram
plt.figure(figsize=(6, 4))
plt.hist(df['age'], color='green', bins=20, edgecolor='black')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Detailed Age Distribution')

# Bar
plt.figure(figsize=(6, 4))
gender_counts = df['gender'].value_counts()
plt.bar(gender_counts.index, gender_counts.values, color=['skyblue', 'lightcoral'], width=0.4)
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Gender Comparison')

# Pie
plt.figure(figsize=(6, 4))
plt.pie(
    gender_counts,
    labels=gender_counts.index,
    autopct='%1.1f%%',
    startangle=90,
    colors=['skyblue', 'lightcoral']
)
plt.title('Gender Proportion')

# Scatter
plt.figure(figsize=(6, 4))
plt.scatter(data['age'], data['income'], c='red', edgecolor='black', s=100)
plt.xlabel('Age')
plt.ylabel('Income')
plt.title('Age vs Income')

# Show all plots at once
plt.show()

Lab Experiment 2: Data Cleaning and Pre-processing
Objective: To perform common data cleaning and pre-processing tasks on a dataset. This
includes filling missing values, removing and inserting columns, renaming a target column,
performing feature scaling, and converting categorical values to numerical values.
1. Introduction & Theory
Data pre-processing is a crucial step in the machine learning pipeline that involves transforming
raw data into a clean and understandable format. Without proper pre-processing, a machine
learning model may produce inaccurate or unreliable results.
This experiment will cover several key pre-processing techniques:
â€¢ Handling Missing Values: Datasets often have missing values, which can be handled
by filling them with a statistical value (like the mean or median), using forward or
backward fill, or by dropping the affected rows/columns.
â€¢ Feature Engineering: This involves creating new features or modifying existing ones.
We will practice removing an irrelevant column and creating a new column based on
existing data.
â€¢ Feature Scaling: Many machine learning algorithms perform better when numerical
input variables are scaled to a standard range. We will explore Min-Max Scaling
(normalizing to a [0, 1] range) and Standard Scaling (rescaling to have a mean of 0
and standard deviation of 1).
â€¢ Categorical Data Encoding: Machine learning models require numerical input.
Categorical data (like 'gender' or 'marital_status') must be converted into a numerical
format. We will demonstrate Label Encoding, which assigns a unique integer to each
category.
7. Conclusion
This experiment successfully demonstrated several essential data cleaning and pre-processing
techniques. We handled missing data using statistical imputation and forward-fill, engineered
new features, applied both Min-Max and Standard scaling to normalize data, and encoded
categorical variables into a numerical format. These steps are foundational for preparing a
dataset for effective machine learning model training and ensure data quality and consistency.
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from IPython.display import display  # ðŸ‘ˆ for showing DataFrames properly in Jupyter

# Generate random data
data = {
    'age': [35, 41, 23, 32, 28, 36, 45, 39, 44, 29],
    'income': ['70000', '90000', '50000', '60000', '', '75000', '100000', '80000', '95000', '55000'],
    'date': [
        '2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05',
        '2020-01-06', '2020-01-07', '', '2020-01-09', '2020-01-10'
    ],
    'marital_status': [
        'married', 'single', 'married', 'single', 'married',
        'single', 'married', 'single', 'married', 'single'
    ],
    'gender': ['female', 'male', 'male', 'female', 'female', 'male', 'female', 'male', 'male', 'female']
}

# Create DataFrame
df = pd.DataFrame(data)
df.to_csv('data.csv', index=False)

# Reload
df = pd.read_csv('data.csv')

# Replace empty strings with NaN
df.replace('', np.nan, inplace=True)

# Convert income to numeric
df['income'] = pd.to_numeric(df['income'], errors='coerce')

# Fill missing values
df['income'].fillna(df['income'].mean(), inplace=True)
df['date'].fillna(method='ffill', inplace=True)
df.dropna(inplace=True)

# Add computed column
df['age_squared'] = df['age'] ** 2

print("âœ… Original cleaned DataFrame:")
display(df)

# Copy for transformation
df1 = df.copy()

# Rename income column
df1 = df1.rename(columns={'income': 'annual_income'})

print("\nColumns after rename:")
display(df1.columns.to_list())

# Apply Min-Max scaling to 'age'
scaler = MinMaxScaler()
df1['age'] = scaler.fit_transform(df1[['age']])

# Apply Standard scaling to 'annual_income' and 'age_squared'
scaler = StandardScaler()
df1[['annual_income', 'age_squared']] = scaler.fit_transform(df1[['annual_income', 'age_squared']])

print("\nâœ… Scaled DataFrame:")
display(df1)

# One-hot encoding for categorical columns
df_dummies = pd.get_dummies(df, columns=['marital_status', 'gender'])
print("\nâœ… One-hot Encoded DataFrame:")
display(df_dummies)

# Label encoding for categorical columns
le = LabelEncoder()
df['marital_status_encoded'] = le.fit_transform(df['marital_status'])
df['gender_encoded'] = le.fit_transform(df['gender'])

print("\nâœ… Label Encoded DataFrame:")
display(df)

Lab Experiment 3: Simple Linear Regression
Objective: To build a simple linear regression model to predict sales based on the money spent
on different marketing platforms. Implement an ordinary least squares (OLS) linear regression
and evaluate the accuracy of the model using Root Mean Squared Error (RMSE) and R-squared
metrics.
1. Introduction & Theory
Linear Regression is a fundamental supervised machine learning algorithm used for modeling
the relationship between a dependent variable (target) and one or more independent variables
(features). In simple linear regression, we analyze the relationship between a single
independent variable (X) and a dependent variable (Y). The core assumption is that a linear
relationship exists between X and Y, which can be modeled by fitting a straight line that best
describes this relationship. The equation for a simple linear regression line is: Where:
â€¢ Y is the dependent variable (target).
â€¢ X is the independent variable (feature).
â€¢ is the intercept of the line (the value of Y when X is 0).
â€¢ is the slope of the line (the change in Y for a one-unit change in X).
â€¢ is the error term, representing the difference between the actual and predicted values.
In this experiment, we will use an advertising dataset to predict Sales based on the advertising
budget for TV.
Conclusion:
In this experiment, we successfully built and evaluated a simple linear regression model to
predict sales from TV advertising expenditure. The correlation analysis confirmed that TV
advertising has a strong positive relationship with sales. The model achieved an R-squared
value of approximately 0.84, indicating that about 84% of the variability in sales can be
explained by the TV advertising budget. The RMSE of 2.15 provides a measure of the average
prediction error. The visualization of actual vs. predicted values shows a strong linear trend,
confirming that the model provides a good fit for the data
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Step 1: Create a sample dataset
data = {
    'TV': [230.1, 44.5, 17.2, 151.5, 180.8, 8.7, 57.5, 120.2, 8.6, 199.8,
           66.1, 214.7, 23.8, 97.5, 204.1, 195.4, 67.8, 281.4, 69.2, 147.3],
    'Sales': [22.1, 10.4, 9.3, 18.5, 12.9, 7.2, 11.8, 13.2, 4.8, 15.6,
              12.6, 17.4, 9.2, 9.7, 19.0, 22.4, 12.5, 24.4, 11.3, 14.6]
}
df = pd.DataFrame(data)
# df = pd.read_csv("advertising.csv")

print("Dataset Preview:")
print(df.head(), "\n")

# Step 2: Data check
print("Dataset shape:", df.shape)
print("\nMissing values:\n", df.isnull().sum())

# Step 3: Feature selection
X = df[['TV']]
y = df['Sales']

# Step 4: Split into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)

# Step 5: Build and train the linear regression model
model = LinearRegression()
model.fit(x_train, y_train)

# Step 6: Make predictions
y_pred = model.predict(x_test)

# Step 7: Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared: {r2:.4f}\n")

# Step 8: Visualize results
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.xlabel('Actual Sales')
plt.ylabel('Predicted Sales')
plt.title('Linear Regression: Actual vs Predicted')
plt.show()


Lab Experiment 4: Multiple Linear Regression
Objective: To build a multiple linear regression model to predict sales based on the money
spent on multiple marketing platforms (TV, Radio, and Newspaper). The accuracy of the model
will be evaluated using Root Mean Squared Error (RMSE) and R-squared metrics.
1. Introduction & Theory
Multiple Linear Regression is an extension of simple linear regression. It is a statistical
technique used to model the relationship between a single dependent variable (target) and two
or more independent variables (features). The goal is to find a linear equation that best predicts
the value of the dependent variable based on the values of the independent variables. The
equation for a multiple linear regression model is: Where:
â€¢ Y is the dependent variable (target).
â€¢ are the independent variables (features).
â€¢ is the intercept of the line.
â€¢ are the coefficients for each independent variable, representing the change in Y for a
one-unit change in that variable, holding all others constant.
â€¢ is the model's error term.
In this experiment, we will predict Sales using the advertising budgets for TV, Radio, and
Newspaper as our features
Conclusion
In this experiment, a multiple linear regression model was successfully constructed to predict
sales based on advertising spending across TV, Radio, and Newspaper platforms. The model
demonstrated a strong predictive capability, achieving an R-squared value of approximately
0.91. This means our model can explain about 91% of the variance in sales, a significant
improvement over the simple linear regression model which only used TV. The RMSE of 1.64
is also lower, indicating a smaller average prediction error. The visualization confirms that the
model's predictions are closely aligned with the actual sales figures, making it a robust model
for this task.
# =================================================================
# Step 1: Importing Necessary Libraries
# =================================================================
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
# import seaborn as sns # Used for visualizations/EDA if needed

# Set plot style for better aesthetics
plt.style.use('ggplot') 

# =================================================================
# Step 2: Loading and Inspecting the Dataset
# NOTE: Ensure 'advertising.csv' is in the same directory as your notebook.
# =================================================================
try:
    df = pd.read_csv('advertising.csv')
except FileNotFoundError:
    print("Error: 'advertising.csv' not found. Creating a small dummy dataset for demonstration.")
    # Create a dummy dataset if the file is missing (not part of the original experiment, but helpful)
    data = {
        'TV': np.random.uniform(10, 300, 100),
        'Radio': np.random.uniform(0, 50, 100),
        'Newspaper': np.random.uniform(0, 120, 100)
    }
    dummy_df = pd.DataFrame(data)
    # Simple linear relationship for sales: Sales â‰ˆ 0.05*TV + 0.1*Radio + error
    dummy_df['Sales'] = (0.05 * dummy_df['TV'] + 0.1 * dummy_df['Radio'] + np.random.normal(0, 2, 100))
    df = dummy_df
    
df = pd.DataFrame(df)
print("First 5 rows of the data:")
print(df.head())

# Check for null values and data types
print("\nData Info:")
df.info()

# =================================================================
# Step 3: Model Building and Training
# =================================================================

# 3.1 Preparing the Data (Features X and Target y)
# Features (X): All columns except 'Sales' [cite: 11]
X = df.drop('Sales', axis=1) 
# Target (y): 'Sales' column
y = df['Sales']

# Split data into training (70%) and testing (30%) sets [cite: 11]
# random_state=41 ensures the split is the same every time
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41) 

print(f"\nTraining set shape: {x_train.shape}")
print(f"Testing set shape: {x_test.shape}")

# 3.2 Building and Fitting the Multiple Linear Regression Model
model = LinearRegression()
# Fit the model to the training data [cite: 11]
model.fit(x_train, y_train)

# =================================================================
# Step 4: Model Evaluation
# =================================================================

# 4.1 Making Predictions
y_pred = model.predict(x_test)

# Print Model Coefficients (to see the influence of each feature)
print("\nModel Coefficients:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: {coef:.4f}")
print(f"Intercept: {model.intercept_:.4f}")


# 4.2 Calculating Metrics
# Root Mean Squared Error (RMSE) - Average prediction error 
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"\nRoot Mean Squared Error (RMSE): {rmse:.4f}")

# R-squared - Percentage of variance in sales explained by the model 
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared:.4f}")

# =================================================================
# Step 5: Visualizing the Fit
# =================================================================

plt.figure(figsize=(8, 6))
# Scatter plot of the actual vs. predicted sales 
plt.scatter(y_test, y_pred, color='darkblue', alpha=0.7) 

plt.xlabel('Actual Sales')
plt.ylabel('Predicted Sales')
plt.title('Actual vs. Predicted Sales (Multiple Linear Regression)')

# Add a diagonal line (k--) for perfect prediction for reference 
# Find the min/max limits for the diagonal line
lims = [min(min(y_test), min(y_pred)), max(max(y_test), max(y_pred))]
plt.plot(lims, lims, color='red', linestyle='--', linewidth=2, label='Perfect Fit Line')
plt.grid(True)
plt.legend()
plt.show()

Lab Experiment 5: Logistic Regression for Classification
Objective: To build a logistic regression model to classify whether an employee is suitable for
a promotion or not, based on a given employee dataset. The performance of the model will be
evaluated using a classification report and a confusion matrix.
1. Introduction & Theory
Logistic Regression is a supervised machine learning algorithm primarily used for binary
classification tasks. Although its name includes "regression," it is a classification algorithm. It
is used to predict the probability that an instance belongs to a particular class.
The core idea of logistic regression is to take the output of a linear regression equation and pass
it through a sigmoid function (or logistic function). This function squashes the output value to
a range between 0 and 1, which can be interpreted as a probability. A threshold (commonly 0.5)
is then used to convert this probability into a class prediction (e.g., if probability > 0.5, predict
Class 1; otherwise, predict Class 0).
The sigmoid function is defined as:
1
ð‘ƒ(ð‘Œ = 1) = 1 / 1 + ð‘’^âˆ’ð‘§
ð‘¤â„Žð‘’ð‘Ÿð‘’ ð‘§ = ð›½0 + ð›½1ð‘‹1 + ð›½2ð‘‹2 + .. + ð›½ð‘›ð‘‹ð‘›
In this experiment, we will use a synthetic employee dataset to predict whether an employee
will receive a promotion (the target variable)
Conclusion
In this experiment, a logistic regression model was successfully built to classify whether an
employee is eligible for a promotion. After generating a synthetic dataset, we pre-processed the
data by encoding categorical variables and selecting relevant features. The model was trained
and then evaluated on a test set.
The classification report and confusion matrix provide a detailed overview of the model's
performance. Since the data was generated randomly, the model's accuracy is around 50%,
which is expected for a random baseline. In a real-world scenario with meaningful data
patterns, we would expect a much higher accuracy. This experiment effectively demonstrates
the complete workflow for a binary classification problem using logistic regression
import micropip
await micropip.install("seaborn")
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

# =========================================================
# 1. Setup and Synthetic Dataset Generation
# =========================================================

print("--- Step 1: Generating Synthetic Employee Dataset ---")
np.random.seed(123)
n = 1000 # Number of employees
age = np.random.normal(40, 10, n)
gender = np.random.choice(['male', 'female'], n)
education = np.random.choice(['high school', 'college', 'graduate'], n)
job_level = np.random.choice(['junior', 'senior'], n)
last_evaluation = np.random.uniform(0.4, 1, n)
average_monthly_hours = np.random.randint(100, 300, n)
time_spend_company = np.random.randint(1, 10, n)
number_of_projects = np.random.randint(1, 7, n)
promotion = np.random.choice([0, 1], n) # Target variable (0: No Promotion, 1: Promotion)
work_accident = np.random.choice([0, 1], n)
salary = np.random.choice(['low', 'medium', 'high'], n)

# Create DataFrame
data = pd.DataFrame({
    'age': age,
    'gender': gender,
    'education': education,
    'job_level': job_level,
    'last_evaluation': last_evaluation,
    'average_monthly_hours': average_monthly_hours,
    'time_spend_company': time_spend_company,
    'number_of_projects': number_of_projects,
    'work_accident': work_accident,
    'promotion': promotion,
    'salary': salary
})

# Save and load (for standard lab file procedure)
data.to_csv('Employee.csv', index=False)
data = pd.read_csv('Employee.csv')
print(f"Dataset generated with {len(data)} records.")

# =========================================================
# 2. Data Pre-processing and Splitting
# =========================================================

print("\n--- Step 2: Data Pre-processing ---")
# Label Encoding for categorical features
le = LabelEncoder()
categorical_cols = ['gender', 'education', 'job_level', 'salary']
data[categorical_cols] = data[categorical_cols].apply(le.fit_transform)

# Feature Selection (dropping selected columns)
data = data.drop([
    'last_evaluation', 
    'number_of_projects', 
    'average_monthly_hours', 
    'time_spend_company', 
    'work_accident'
], axis=1)
print("Categorical features encoded and irrelevant features dropped.")

# Define features (X) and target (y)
X = data.drop('promotion', axis=1)
y = data['promotion']

# Train-Test Split (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
print(f"Data split: Training set size {X_train.shape[0]}, Test set size {X_test.shape[0]}.")

# =========================================================
# 3. Model Training, Prediction, and Evaluation
# =========================================================

print("\n--- Step 3: Training Logistic Regression Model ---")
# Building and Fitting the Logistic Regression Model
model = LogisticRegression(solver='liblinear', random_state=42)
model.fit(X_train, y_train)
print("Model training complete.")

# Making Predictions
y_pred = model.predict(X_test)

# Evaluation - Classification Report
print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

# Evaluation - Confusion Matrix Plot
print("--- Generating Confusion Matrix Plot ---")
cm = confusion_matrix(y_test, y_pred)
class_names = ['Not Promoted (0)', 'Promoted (1)']

# Plot the Confusion Matrix
plt.figure(figsize=(6, 5))
ax = plt.subplot()
sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt='g')

# Labels and Title
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix for Employee Promotion')
ax.xaxis.set_ticklabels(class_names)
ax.yaxis.set_ticklabels(class_names)
plt.show()

print("\n--- Experiment Complete ---")

Lab Experiment 6: Naive Bayes Classifier for Spam Detection
Objective: To build a Naive Bayes classifier to identify and filter SMS spam messages. The
performance of the model will be evaluated using accuracy and a confusion matrix.
1. Introduction & Theory
Naive Bayes is a supervised machine learning algorithm based on the Bayes theorem. It is a
probabilistic classifier, meaning it makes predictions based on the probability of an object
belonging to a certain class. The "naive" part of the name comes from its core assumption: the
features it uses for classification are all independent of each other, given the class. The theorem
is stated as:
â€¢ P (A | B) is the posterior probability: the probability of hypothesis A being true, given
the evidence B.
â€¢ P (B | A) is the likelihood: the probability of observing evidence B, given that
hypothesis A is true.
â€¢ P(A) is the prior probability: the initial probability of hypothesis A.
â€¢ P(B) is the marginal likelihood: the probability of observing evidence B.
Naive Bayes is particularly effective for text classification tasks like spam detection because it
can handle a large number of features (i.e., the vocabulary of words in a text corpus) efficiently.
In this experiment, we will use the Multinomial Naive Bayes variant, which is well-suited for
classification with discrete features (like word counts).
From the confusion matrix, we can observe:
â€¢ True Negatives (Top-Left): 1015 "ham" messages were correctly classified as "ham".
â€¢ False Positives (Top-Right): 8 "ham" messages were incorrectly classified as "spam".
â€¢ False Negatives (Bottom-Left): 8 "spam" messages were incorrectly classified as
"ham".
â€¢ True Positives (Bottom-Right): 141 "spam" messages were correctly classified as
"spam".
6. Conclusion
The classifier achieved an impressive accuracy of approximately 98.6% on the test set. The
confusion matrix further confirmed the model's high performance, with very few
misclassifications. This demonstrates that Naive Bayes is a highly effective and efficient
algorithm for text classification problems like spam detection.
import micropip
await micropip.install("seaborn")
# Lab Experiment 6: Naive Bayes Classifier for Spam Detection (Self-contained version)

# Step 1: Import Necessary Libraries
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Create a Synthetic Dataset (Simulating spam.csv)
# Each text is short and simple to reflect "ham" or "spam" type SMS
messages = [
    "Win a free iPhone now!", "Hey, are we still on for dinner tonight?",
    "Congratulations! You have won a lottery of $1000", "Don't forget to bring your notebook tomorrow",
    "URGENT! Your account has been compromised. Click here to verify.", "Let's catch up soon!",
    "Youâ€™ve been selected for a cash prize, claim now", "Can you send me the notes please?",
    "FREE entry in a weekly draw to win a car!", "Lunch at 1pm?",
    "Limited offer! Buy one get one free!", "Meeting postponed to 3pm",
    "Claim your reward now by replying YES", "Iâ€™ll call you in 10 minutes",
    "Earn money working from home. Apply today!", "Assignment submission deadline tomorrow",
    "Exclusive deal just for you", "See you in class",
    "Get 70% discount on all items this weekend", "Please check your email"
]

labels = [
    "spam", "ham", "spam", "ham", "spam", "ham",
    "spam", "ham", "spam", "ham", "spam", "ham",
    "spam", "ham", "spam", "ham", "spam", "ham",
    "spam", "ham"
]

# Combine into a DataFrame
data = pd.DataFrame({
    'label': labels,
    'text': messages
})

# Save and reload to follow standard lab file procedure
data.to_csv('spam.csv', index=False)
data = pd.read_csv('spam.csv')

print("Dataset Preview:")
print(data.head(), "\n")

# Step 3: Data Pre-processing â€” Train-Test Split
train_data = data[:14]
test_data = data[14:]

# Step 4: Text Vectorization using CountVectorizer
vectorizer = CountVectorizer()
train_vectors = vectorizer.fit_transform(train_data["text"])
test_vectors = vectorizer.transform(test_data["text"])

# Step 5: Model Building and Training
nb_classifier = MultinomialNB()
nb_classifier.fit(train_vectors, train_data["label"])

# Step 6: Making Predictions
predictions = nb_classifier.predict(test_vectors)

# Step 7: Model Evaluation
accuracy = accuracy_score(test_data["label"], predictions)
print(f"Model Accuracy: {accuracy:.4f}\n")

# Step 8: Confusion Matrix Visualization
cm = confusion_matrix(test_data["label"], predictions)
ax = plt.subplot()
sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt='g')

# Add labels and titles
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix - Naive Bayes Spam Detection')
ax.xaxis.set_ticklabels(['ham', 'spam'])
ax.yaxis.set_ticklabels(['ham', 'spam'])
plt.show()

Experiment 7: K-Nearest Neighbors (KNN) Algorithm for Classification
Objective: To implement the K-Nearest Neighbors (KNN) algorithm to classify the species of
iris flowers from the Iris dataset. The performance of the model will be evaluated using
accuracy, a confusion matrix, and a classification report.
1. Introduction & Theory
The K-Nearest Neighbors (KNN) algorithm is a simple, yet powerful, supervised machine
learning algorithm used for both classification and regression tasks. It is a non-parametric,
instance-based learning algorithm, which means it does not make any assumptions about the
underlying data distribution.
The core principle of KNN is to classify a new, unseen data point based on the majority class
of its 'K' nearest neighbors in the feature space. The "neighbors" are the existing data points
from the training set, and the "distance" between them is typically calculated using a distance
metric, most commonly the Euclidean distance. The algorithm works as follows:
1. Choose a value for K: K represents the number of nearest neighbors to consider.
2. Calculate Distances: For a new data point, calculate the distance to all points in the
training dataset.
3. Find K-Nearest Neighbors: Identify the K training data points that are closest to the
new point.
4. Majority Vote: For a classification task, assign the new data point to the class that is
most common among its K-nearest neighbors.
In this experiment, we will use the famous Iris dataset to classify flowers into one of three
species: Iris Setosa, Iris Versicolour, or Iris Virginica
 Conclusion
In this experiment, the K-Nearest Neighbors algorithm was successfully implemented to
classify species of iris flowers. The model was trained on 80% of the dataset and tested on the
remaining 20%.
With K=3, the model achieved a perfect accuracy of 1.0 (or 100%) on the test set, as confirmed
by the accuracy score, confusion matrix, and classification report. This indicates that for this
particular train-test split and choice of K, the model was able to perfectly distinguish between
the three iris species. The Iris dataset is a well-known, clean dataset, and high accuracy is often
achievable. This experiment effectively demonstrates the simplicity and power of the KNN
algorithm for multi-class classification tasks.
import micropip
await micropip.install("seaborn")
# Lab Experiment 7: K-Nearest Neighbors (KNN) Algorithm for Classification
# Self-contained version for Jupyter Notebook

# Step 1: Import Necessary Libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Load and Save Dataset (Following Standard Lab File Procedure)
iris = load_iris()

# Create a DataFrame for better preview
data = pd.DataFrame(iris["data"], columns=iris["feature_names"])
data["target"] = iris["target"]

# Save and reload dataset
data.to_csv('iris.csv', index=False)
data = pd.read_csv('iris.csv')

print("Dataset Preview:")
print(data.head(), "\n")
print("Shape of dataset:", data.shape, "\n")

# Step 3: Prepare the Data
X = data.iloc[:, :-1]   # Feature columns
y = data.iloc[:, -1]    # Target column

# Split dataset into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape, "\n")

# Step 4: Model Building and Training
# Create KNN classifier with K=3
knn = KNeighborsClassifier(n_neighbors=3)

# Train the model
knn.fit(X_train, y_train)

# Step 5: Model Evaluation
# Make predictions on test data
predictions = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, predictions)
print(f"Model Accuracy: {accuracy:.4f}\n")

# Display classification report
print("Classification Report:")
print(classification_report(y_test, predictions))

# Step 6: Confusion Matrix
cm = confusion_matrix(y_test, predictions)
ax = plt.subplot()
sns.heatmap(cm, annot=True, ax=ax, cmap='Greens', fmt='g')

# Add labels and title
ax.set_xlabel('Predicted Labels')
ax.set_ylabel('True Labels')
ax.set_title('Confusion Matrix - KNN Classifier')
ax.xaxis.set_ticklabels(iris["target_names"])
ax.yaxis.set_ticklabels(iris["target_names"])
plt.show()

Lab Experiment 8: Decision Tree Classifier
Objective: To implement a Decision Tree Classifier to classify the species of iris flowers from
the Iris dataset. The performance of the model will be evaluated, and hyperparameter tuning
will be performed to find the optimal tree depth.
1. Introduction & Theory
A Decision Tree is a supervised machine learning algorithm that is widely used for both
classification and regression tasks. It is a tree-based model that functions like a flowchart,
where each internal node represents a test on a feature, each branch represents the outcome of
the test, and each leaf node represents a class label (decision).
In this experiment, we will use the Iris dataset to train a Decision Tree Classifier to distinguish
between the three different species of Iris flowers based on their sepal and petal measurements
Conclusion
In this experiment, a Decision Tree Classifier was successfully implemented and trained to
classify the Iris flower species. After normalizing the feature data, the model was trained and
evaluated, achieving a perfect accuracy of 100% on the test set.
Furthermore, we performed hyperparameter tuning using GridSearchCV to find the optimal
max_depth for the tree, which was determined to be 4. This demonstrates not only how to build
a decision tree model but also how to optimize it for better generalization. The high accuracy
confirms the effectiveness of the Decision Tree algorithm for this well-structured dataset.

import micropip
await micropip.install("seaborn")
# Lab Experiment 8: Decision Tree Classifier
# Self-contained version for Jupyter Notebook

# Step 1: Import Necessary Libraries
from sklearn.datasets import load_iris
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Load and Save Dataset (Standard Lab File Procedure)
iris = load_iris()

# Create a DataFrame for better visualization
data = pd.DataFrame(iris["data"], columns=iris["feature_names"])
data["target"] = iris["target"]

# Save and reload dataset to maintain lab procedure
data.to_csv('iris.csv', index=False)
data = pd.read_csv('iris.csv')

print("Dataset Preview:")
print(data.head(), "\n")
print("Shape of dataset:", data.shape, "\n")

# Step 3: Data Preprocessing

# 3.1 Normalize the feature data using MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(data.iloc[:, :-1])
y = data["target"]

# 3.2 Split data into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape, "\n")

# Step 4: Model Building and Training
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Step 5: Model Evaluation

# 5.1 Make predictions
y_pred = clf.predict(X_test)

# 5.2 Calculate and display accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}\n")

# 5.3 Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
ax = plt.subplot()
sns.heatmap(cm, annot=True, ax=ax, cmap='Oranges', fmt='g')

# Add labels and title
ax.set_xlabel('Predicted Labels')
ax.set_ylabel('True Labels')
ax.set_title('Confusion Matrix - Decision Tree Classifier')
ax.xaxis.set_ticklabels(iris["target_names"])
ax.yaxis.set_ticklabels(iris["target_names"])
plt.show()

# 5.4 Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris["target_names"]))

# Step 6: Visualizing the Decision Tree
plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=iris["feature_names"], class_names=iris["target_names"], filled=True)
plt.title("Decision Tree Visualization")
plt.show()

# Step 7: Hyperparameter Tuning using GridSearchCV
params = {'max_depth': [1, 2, 3, 4, 5, None]}
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid=params, cv=5)
grid_search.fit(X_train, y_train)

print(f"Best Parameters from Grid Search: {grid_search.best_params_}\n")

Lab Experiment 9: Dimensionality Reduction using Principal Component Analysis
Objective: To perform dimensionality reduction on a high-dimensional dataset using the
Principal Component Analysis (PCA) algorithm, retaining a specified percentage of the
variance from the original data.
1. Introduction & Theory
Principal Component Analysis (PCA) is a widely used unsupervised machine learning
algorithm for dimensionality reduction. In many real-world datasets, features are often highly
correlated, leading to redundancy and increased model complexity (the "curse of
dimensionality"). PCA addresses this by transforming the original, correlated features into a
new set of linearly uncorrelated variables called principal components.
The principal components are ordered by the amount of variance they explain in the original
data. The first principal component accounts for the largest possible variance, the second
component accounts for the largest remaining variance, and so on. By selecting a subset of
these principal components (e.g., the top 'k' components that explain 95% of the total variance),
we can reduce the number of features in the dataset while retaining most of the important
information.
This technique is valuable for data visualization, speeding up model training, and mitigating
overfitting. In this experiment, we will apply PCA to a synthetic 10-dimensional dataset to
reduce its dimensionality.
Conclusion
In this experiment, we successfully applied Principal Component Analysis to reduce the
dimensionality of a 10-feature dataset. By analyzing the explained variance, we determined
that 9 principal components were sufficient to capture at least 85% of the information from the
original data.
The original dataset was then transformed into this new, lower-dimensional space. This reduced
dataset can now be used for training machine learning models more efficiently, with less risk
of overfitting and faster computation times, while still retaining the original variance.

import micropip
await micropip.install("seaborn")
# Lab Experiment 9: Dimensionality Reduction using Principal Component Analysis (PCA)
# Self-contained version for Jupyter Notebook

# Step 1: Import Necessary Libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Step 2: Generate and Save Synthetic Dataset (Standard Lab Procedure)
# Create a synthetic dataset with 1000 samples and 10 features
np.random.seed(42)
data = np.random.randn(1000, 10)

# Convert to DataFrame
df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(1, 11)])

# Add a binary label column
df['label'] = np.random.randint(0, 2, size=1000)

# Save and reload the dataset to maintain lab file procedure
df.to_csv('data.csv', index=False)
df = pd.read_csv('data.csv')

print("Dataset Preview:")
print(df.head(), "\n")
print("Shape of dataset:", df.shape, "\n")

# Step 3: Data Pre-processing

# 3.1 Separate features and label
X = df.drop('label', axis=1)
y = df['label']

# 3.2 Standardize the feature data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Data successfully standardized.\n")

# Step 4: Applying PCA

# 4.1 Apply PCA with all components
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# 4.2 Get explained variance ratio
explained_variance = pca.explained_variance_ratio_
cum_var_exp = np.cumsum(explained_variance)

# Step 5: Visualizing Explained Variance

plt.figure(figsize=(8, 5))
plt.bar(range(1, 11), explained_variance, alpha=0.6, label='Individual Explained Variance')
plt.step(range(1, 11), cum_var_exp, where='mid', label='Cumulative Explained Variance')
plt.xlabel('Principal Component Index')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance by Principal Components')
plt.legend(loc='best')
plt.show()

# Step 6: Determine Number of Components for 85% Variance
n_components = np.argmax(cum_var_exp >= 0.85) + 1
print(f"Number of components to retain (â‰¥85% variance): {n_components}\n")

# Step 7: Transform the Dataset using Optimal Components
pca_final = PCA(n_components=n_components)
X_reduced = pca_final.fit_transform(X_scaled)

# Create a new DataFrame with the reduced data
reduced_df = pd.concat([pd.DataFrame(X_reduced, columns=[f'PC{i}' for i in range(1, n_components + 1)]), y], axis=1)

print("Reduced Dataset Preview:")
print(reduced_df.head(), "\n")
print("Shape of reduced dataset:", reduced_df.shape, "\n")

# Step 8: Visualization of Transformed Data (First Two Components)
plt.figure(figsize=(7, 5))
plt.scatter(reduced_df['PC1'], reduced_df['PC2'], c=reduced_df['label'], cmap='coolwarm', alpha=0.7)
plt.title('2D Visualization of Data after PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

Lab Experiment 10: K-Means Clustering
Objective: To apply the K-Means clustering algorithm to the Wine dataset to group similar
wines into clusters based on their chemical analysis. The optimal number of clusters will be
determined using the Elbow Method.
1. Introduction & Theory
Clustering is a type of unsupervised machine learning where the goal is to partition a dataset
into groups (or "clusters"). Data points within the same cluster are more similar to each other
than to those in other clusters. Unlike supervised learning, clustering algorithms work with
unlabeled data.
K-Means is one of the most popular and straightforward clustering algorithms. It aims to
partition n observations into K clusters, where each observation belongs to the cluster with the
nearest mean (cluster centroid). The algorithm works as follows:
1. Choose the number of clusters (K): This is a predefined hyperparameter.
2. Initialize Centroids: Randomly select K data points from the dataset to serve as the
initial cluster centroids.
3. Assign Clusters: Assign each data point to the cluster whose centroid is closest (usually
determined by Euclidean distance).
4. Update Centroids: Recalculate the centroid of each cluster by taking the mean of all
data points assigned to it.
5. Repeat: Repeat steps 3 and 4 until the cluster assignments no longer change or a
maximum number of iterations is reached.
In this experiment, we will apply K-Means to the Wine dataset, which contains the results of a
chemical analysis of wines derived from three different cultivars.
Conclusion
In this experiment, the K-Means clustering algorithm was successfully applied to the Wine
dataset. After scaling the features, the Elbow Method was used to identify the optimal number
of clusters, which was found to be 3.
The K-Means model was then trained with K=3, and the resulting clusters were visualized. The
algorithm effectively partitioned the data into three groups based on their chemical properties,
demonstrating its utility as an unsupervised learning technique for discovering underlying
structures in a dataset. This experiment highlights the full process of applying K-Means, from
data preparation and hyperparameter tuning to final model fitting and evaluation

import micropip
await micropip.install("seaborn")
# Lab Experiment 10: K-Means Clustering
# Self-contained version for Jupyter Notebook

# Step 1: Import Necessary Libraries
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.datasets import load_wine
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Step 2: Load and Save Dataset (Standard Lab File Procedure)
wine_data = load_wine()

# Create DataFrame for clarity
data = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)

# Save and reload to maintain lab format
data.to_csv('wine.csv', index=False)
data = pd.read_csv('wine.csv')

print("Dataset Preview:")
print(data.head(), "\n")
print("Shape of dataset:", data.shape, "\n")

# Step 3: Data Pre-processing
# 3.1 Feature Scaling using StandardScaler
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

print("Data successfully standardized.\n")

# Step 4: Finding Optimal Number of Clusters using Elbow Method
wss_values = []

for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    kmeans.fit(data_scaled)
    wss_values.append(kmeans.inertia_)

# Plot the Elbow Curve
plt.figure(figsize=(7, 5))
plt.plot(range(1, 11), wss_values, marker='o')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Within-Cluster Sum of Squares (WSS)')
plt.show()

# Step 5: Choosing Optimal K (Visually observed from elbow = 3)
optimal_k = 3
print(f"Optimal number of clusters (K) selected: {optimal_k}\n")

# Step 6: Apply K-Means with Optimal K
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')
kmeans.fit(data_scaled)

# Add cluster labels to DataFrame
data['Cluster'] = kmeans.labels_

print("Cluster assignment completed.\n")
print(data[['Cluster']].head(), "\n")

# Step 7: Visualizing Clusters (Using first two features for clarity)
plt.figure(figsize=(7, 5))
sns.scatterplot(
    x=data_scaled[:, 0],
    y=data_scaled[:, 1],
    hue=data['Cluster'],
    palette='Set1',
    s=50
)
plt.title(f'K-Means Clustering Visualization (K={optimal_k})')
plt.xlabel(wine_data.feature_names[0])
plt.ylabel(wine_data.feature_names[1])
plt.legend(title='Cluster')
plt.show()

# Step 8: Cluster Summary
print("Cluster Centers (in scaled feature space):\n")
centers = pd.DataFrame(kmeans.cluster_centers_, columns=wine_data.feature_names)
print(centers, "\n")


